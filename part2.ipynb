{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2709cb",
   "metadata": {},
   "source": [
    "## Part 2. Model Training & Evaluation - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe0d86",
   "metadata": {},
   "source": [
    "Now with the pretrained word embeddings acquired from Part 1 and the dataset acquired from Part\n",
    "0, you need to train a deep learning model for topic classification using the training set, conforming\n",
    "to these requirements:\n",
    "- Use the pretrained word embeddings from Part 1 as inputs, together with your implementation\n",
    "in mitigating the influence of OOV words; make them learnable parameters during training\n",
    "(they are updated).\n",
    "- Design a simple recurrent neural network (RNN), taking the input word embeddings, and\n",
    "predicting a topic label for each sentence. To do that, you need to consider how to aggregate\n",
    "the word representations to represent a sentence.\n",
    "- Use the validation set to gauge the performance of the model for each epoch during training.\n",
    "You are required to use accuracy as the performance metric during validation and evaluation.\n",
    "- Use the mini-batch strategy during training. You may choose any preferred optimizer (e.g.,\n",
    "SGD, Adagrad, Adam, RMSprop). Be careful when you choose your initial learning rate and\n",
    "mini-batch size. (You should use the validation set to determine the optimal configuration.)\n",
    "Train the model until the accuracy score on the validation set is not increasing for a few\n",
    "epochs.\n",
    "- Try different regularization techniques to mitigate overfitting.\n",
    "- Evaluate your trained model on the test dataset, observing the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598087e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linnsheng/Desktop/NTU/S3/Y1/NLP/SC4002/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from torchtext import data, datasets\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils.config import Config\n",
    "from utils.train import train_rnn_model_with_parameters\n",
    "from utils.helper import SentenceDataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb62726",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', tokenizer_language='en_core_web_sm', include_lengths=True)\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb67032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state=random.seed(Config.SEED), split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4453131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.300d\")\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c9fd6",
   "metadata": {},
   "source": [
    "### Import the embedding matrix and vocab index mapping (train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611ede54",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = Path(\"models/embedding_matrix.npy\")\n",
    "index_from_word_path = Path(\"models/index_from_word.json\")\n",
    "\n",
    "embedding_matrix = np.load(embedding_path)\n",
    "with index_from_word_path.open() as f:\n",
    "    index_from_word = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01798970",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentenceDataset(train_data.examples, index_from_word, LABEL.vocab)\n",
    "valid_dataset = SentenceDataset(valid_data.examples, index_from_word, LABEL.vocab)\n",
    "test_dataset = SentenceDataset(test_data.examples, index_from_word, LABEL.vocab)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4679fe9",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84683b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_SPACE = {\n",
    "    \"batch_size\": [32, 64, 128, 256, 512, 1024, 2048],\n",
    "    \"learning_rate\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    \"optimizer_name\": [\"SGD\", \"Adagrad\", \"RMSprop\", \"Adam\"],\n",
    "    \"hidden_dim\": [256, 128, 64, 32],\n",
    "    \"num_layers\": [1, 2, 4],\n",
    "    \"sentence_representation_type\": [\"last\", \"average\", \"max\"],\n",
    "}\n",
    "all_combinations = list(itertools.product(\n",
    "    SEARCH_SPACE[\"batch_size\"],\n",
    "    SEARCH_SPACE[\"learning_rate\"],\n",
    "    SEARCH_SPACE[\"optimizer_name\"],\n",
    "    SEARCH_SPACE[\"hidden_dim\"],\n",
    "    SEARCH_SPACE[\"num_layers\"],\n",
    "    SEARCH_SPACE[\"sentence_representation_type\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/home/linnsheng/Desktop/NTU/S3/Y1/NLP/SC4002/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'rnn_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['rnn_model'])`.\n",
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=256, num_layers=1, sentence_repr=last\n",
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_256-num_layers_1-sr_type_last-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=256, num_layers=1, sentence_repr=average\n",
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_256-num_layers_1-sr_type_average-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=256, num_layers=1, sentence_repr=max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_256-num_layers_1-sr_type_max-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=256, num_layers=2, sentence_repr=last\n",
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_256-num_layers_2-sr_type_last-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=256, num_layers=2, sentence_repr=average\n",
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_256-num_layers_2-sr_type_average-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=256, num_layers=2, sentence_repr=max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_256-num_layers_2-sr_type_max-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=256, num_layers=4, sentence_repr=last\n",
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_256-num_layers_4-sr_type_last-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=256, num_layers=4, sentence_repr=average\n",
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_256-num_layers_4-sr_type_average-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=256, num_layers=4, sentence_repr=max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_256-num_layers_4-sr_type_max-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=128, num_layers=1, sentence_repr=last\n",
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_128-num_layers_1-sr_type_last-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=128, num_layers=1, sentence_repr=average\n",
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_128-num_layers_1-sr_type_average-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=128, num_layers=1, sentence_repr=max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_128-num_layers_1-sr_type_max-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=128, num_layers=2, sentence_repr=last\n",
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_128-num_layers_2-sr_type_last-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=128, num_layers=2, sentence_repr=average\n",
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_128-num_layers_2-sr_type_average-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=128, num_layers=2, sentence_repr=max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/linnsheng/Desktop/NTU/S3/Y1/NLP/SC4002/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name   | Type               | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model  | RNN                | 2.6 M  | train\n",
      "1 | metric | MulticlassAccuracy | 0      | train\n",
      "------------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.363    Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_128-num_layers_2-sr_type_max-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=128, num_layers=4, sentence_repr=last\n",
      "[Skipping] rnn/test/batch_size_32-lr_1e-05-optimizer_SGD-hidden_dim_128-num_layers_4-sr_type_last-freeze_False-rnn_type_RNN-bidirectional_False\n",
      "Training with configuration: batch_size=32, lr=1e-05, optimizer=SGD, hidden_dim=128, num_layers=4, sentence_repr=average\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linnsheng/Desktop/NTU/S3/Y1/NLP/SC4002/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:428: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linnsheng/Desktop/NTU/S3/Y1/NLP/SC4002/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:428: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 137/137 [00:07<00:00, 19.46it/s, v_num=0, train_loss=1.810, train_acc=0.200]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1103 02:29:28.484923155 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n",
      "[W1103 02:29:28.485076206 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n",
      "[W1103 02:29:28.486456686 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n",
      "[W1103 02:29:28.487100243 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n",
      "[W1103 02:29:28.500474867 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n",
      "[W1103 02:29:28.524891308 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n",
      "[W1103 02:29:28.528424809 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n",
      "[W1103 02:29:28.535858350 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/137 [00:00<?, ?it/s, v_num=0, train_loss=1.800, train_acc=0.333, val_loss=1.790, val_acc=0.187]          "
     ]
    }
   ],
   "source": [
    "for batch_size, lr, optimizer_name, hidden_dim, num_layers, sr_type in all_combinations:\n",
    "    print(f\"Training with configuration: batch_size={batch_size}, lr={lr}, optimizer={optimizer_name}, \"\n",
    "          f\"hidden_dim={hidden_dim}, num_layers={num_layers}, sentence_repr={sr_type}\")\n",
    "\n",
    "    # train_rnn_model_with_parameters expects dataset objects (not DataLoader instances),\n",
    "    # so pass the SentenceDataset instances and let the function create DataLoaders internally.\n",
    "    # Set num_workers=0 to avoid multiprocessing DataLoader worker issues (resize storage error).\n",
    "    # train_rnn_model_with_parameters(\n",
    "    #     embedding_matrix=embedding_matrix,\n",
    "    #     train_dataset=train_dataset,\n",
    "    #     val_dataset=valid_dataset,\n",
    "    #     batch_size=batch_size,\n",
    "    #     learning_rate=lr,\n",
    "    #     optimizer_name=optimizer_name,\n",
    "    #     hidden_dim=hidden_dim,\n",
    "    #     num_layers=num_layers,\n",
    "    #     sentence_representation_type=sr_type,\n",
    "    #     freeze_embedding=False,\n",
    "    #     show_progress=True,\n",
    "    # )\n",
    "    train_rnn_model_with_parameters(\n",
    "        embedding_matrix=embedding_matrix,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=lr,\n",
    "        optimizer_name=optimizer_name,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        sentence_representation_type=sr_type,\n",
    "        freeze_embedding=False,\n",
    "        show_progress=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4324dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_dim in SEARCH_SPACE[\"hidden_dim\"]:\n",
    "    for num_layers in SEARCH_SPACE[\"num_layers\"]:\n",
    "        for optimizer_name in SEARCH_SPACE[\"optimizer_name\"]:\n",
    "            for batch_size in SEARCH_SPACE[\"batch_size\"]:\n",
    "                for learning_rate in SEARCH_SPACE[\"learning_rate\"]:\n",
    "                    for sentence_representation_type in SEARCH_SPACE[\"sentence_representation_type\"]:\n",
    "                        log_message = f\"---------- batch_size_{batch_size}; lr_{learning_rate}; optimizer_{optimizer_name}; hidden_dim_{hidden_dim}; num_layers_{num_layers}; sentence_representation_{sentence_representation_type}  ----------\"\n",
    "                        print(log_message)\n",
    "                        train_rnn_model_with_parameters(\n",
    "                            embedding_matrix=embedding_matrix,\n",
    "                            train_dataset=train_data,\n",
    "                            val_dataset=valid_data,\n",
    "                            batch_size=batch_size,\n",
    "                            learning_rate=learning_rate,\n",
    "                            optimizer_name=optimizer_name,\n",
    "                            hidden_dim=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            sentence_representation_type=sentence_representation_type,\n",
    "                            show_progress=True,\n",
    "                            freeze_embedding=False,\n",
    "                        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
